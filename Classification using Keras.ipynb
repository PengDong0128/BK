{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Bankruptcy Prediction using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2016-12-20, Feng Mai  \n",
    "Use Keras merge layer to combine textual and numerical features for bankruptcy prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding, Flatten, Convolution2D, Convolution1D, Reshape, Lambda, AveragePooling1D, AveragePooling2D, MaxPooling1D\n",
    "from keras.layers import LSTM, SimpleRNN, GRU\n",
    "from keras.regularizers import l1, l2, activity_l2, l1, activity_l1\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.layers import Merge\n",
    "\n",
    "import pickle\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 5000 # max number of words to include (remove lower frequency words)\n",
    "maxlen = 5000  # cut texts after this number of words \n",
    "\n",
    "test_train_split_year = 2011\n",
    "forecast_year = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def n_year_before(df, n = 1):\n",
    "    \"\"\"input x,y df, return df with y and n year before x\"\"\"\n",
    "    dat_tmp = df.copy()\n",
    "    dat_tmp['fyear'] = dat_tmp['fyear'] + n\n",
    "    dat_tmp = dat_tmp.drop('Y',axis =1)\n",
    "    Ys = df[['fyear','gvkey','Y']]\n",
    "    n_year = pd.merge(dat_tmp,Ys,how = 'inner',on=['fyear','gvkey'])\n",
    "    return n_year\n",
    "\n",
    "\n",
    "def pad_text_data():\n",
    "    \"\"\" Load tokenized word sequence and pad it for deep learning\"\"\"\n",
    "    print('Loading data')\n",
    "    X = np.load(\"data/10k/X_keras_unigram.npy\")\n",
    "    \n",
    "    # pad sequence for deep learning\n",
    "    print('Pad sequences')\n",
    "    X = sequence.pad_sequences(X, maxlen=maxlen)\n",
    "    \n",
    "    return X\n",
    "    \n",
    "    \n",
    "def load_data(X_padded_text):\n",
    "    \"\"\" Load tokenized word sequence and pad it for deep learning\"\"\"   \n",
    "    index_10k = pd.read_csv('data/10k/10k_index.csv',usecols=['gvkey','fyear'])\n",
    "    index_10k['index_10k'] = index_10k.index\n",
    "    final_variable = pd.read_csv('data/final_variables.csv')\n",
    "    final_variable = final_variable.drop('Unnamed: 0',1)\n",
    "    final_variable = final_variable.replace([np.inf,-np.inf],0)\n",
    "\n",
    "    # match text index with one year after y, index_10k_y has the index of text data has one year after Y matched\n",
    "    index_10k_y = pd.merge(left=index_10k, right=final_variable, how='inner', on=['gvkey','fyear'])\n",
    "    print(\"Total number of observations with no forecasting: \")\n",
    "    print(index_10k_y.shape)\n",
    "    \n",
    "    index_10k_y_n_year = n_year_before(index_10k_y, n = forecast_year)\n",
    "    print(\"Total number of observations: \")\n",
    "    print(index_10k_y_n_year.shape)\n",
    "    \n",
    "    y = np.array(index_10k_y_n_year['Y'])\n",
    "    X_text = X_padded_text[index_10k_y_n_year['index_10k'].tolist()] # get all text X which has matched one year after Y\n",
    "    X_num = index_10k_y_n_year.drop(['gvkey', 'fyear', 'datadate', 'cusip', 'PERMCO', 'Y', 'PERMNO', 'index_10k'], 1)\n",
    "    X_num = X_num.as_matrix()\n",
    "    # split train-test by year\n",
    "    index_10k_y_n_year = index_10k_y_n_year.reset_index(drop=True)\n",
    "    train_index = index_10k_y_n_year[index_10k_y_n_year['fyear'] < test_train_split_year].index.tolist()\n",
    "    test_index = index_10k_y_n_year[index_10k_y_n_year['fyear'] >= test_train_split_year].index.tolist()\n",
    "    print(X_text.shape, X_num.shape, y.shape)\n",
    "    return X_text, X_num, train_index, test_index, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Pad sequences\n"
     ]
    }
   ],
   "source": [
    "X_padded = pad_text_data()\n",
    "pickle.dump(X_padded, open(\"data/10k/X_padded.pickle\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_padded = pickle.load(open(\"data/10k/X_padded.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of observations with no forecasting: \n",
      "(79222, 44)\n",
      "Total number of observations: \n",
      "(64999, 44)\n",
      "(64999, 5000) (64999, 36) (64999,)\n"
     ]
    }
   ],
   "source": [
    "X_text, X_num, train_index, test_index, y = load_data(X_padded_text = X_padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate a Deep Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define different deep learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def create_model_text(no_merge = False):\n",
    "    # this is the Deep Averaging Network Moodel\n",
    "    # see \"Deep Unordered Composition Rivals Syntactic Methods for Text Classification\", Iyyer et al. 2015\n",
    "\n",
    "    embedding_size = embedding_dims\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, embedding_size, input_length=maxlen))\n",
    "    print(model.layers[-1].output_shape)\n",
    "\n",
    "    model.add(Dropout(0.1)) # Use dropout if implement the DAN model\n",
    "    model.add(AveragePooling1D(pool_length=model.output_shape[1]))\n",
    "    print(model.layers[-1].output_shape)\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # We add a vanilla hidden layer:\n",
    "    model.add(Dense(4))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    if no_merge:\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        print(model.layers[-1].output_shape)\n",
    "\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# def create_model_text(no_merge = False):\n",
    "#     # this is the fast-text model\n",
    "#     model = Sequential()\n",
    "#     # we start off with an efficient embedding layer which maps\n",
    "#     # our vocab indices into embedding_dims dimensions\n",
    "#     model.add(Embedding(max_features,\n",
    "#                         embedding_dims,\n",
    "#                         input_length=maxlen))\n",
    "#     # we add a GlobalAveragePooling1D, which will average the embeddings\n",
    "#     # of all words in the document\n",
    "#     model.add(GlobalAveragePooling1D())\n",
    "#     if no_merge:\n",
    "#         # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "#         model.add(Dense(1, activation='sigmoid'))\n",
    "#         model.compile(loss='binary_crossentropy',\n",
    "#                   optimizer='adam',\n",
    "#                   metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "\n",
    "# def create_model_text(no_merge = False):\n",
    "#     # this is a CNN from Keras example\n",
    "#     nb_filter = 50\n",
    "#     filter_length = 3\n",
    "#     hidden_dims = 8\n",
    "#     model = Sequential()\n",
    "\n",
    "#     # we start off with an efficient embedding layer which maps\n",
    "#     # our vocab indices into embedding_dims dimensions\n",
    "#     model.add(Embedding(max_features,\n",
    "#                         embedding_dims,\n",
    "#                         input_length=maxlen,\n",
    "#                         dropout=0.0))\n",
    "#     # we add a Convolution1D, which will learn nb_filter\n",
    "#     # word group filters of size filter_length:\n",
    "#     model.add(Convolution1D(nb_filter=nb_filter,\n",
    "#                             filter_length=filter_length,\n",
    "#                             border_mode='valid',\n",
    "#                             activation='relu',\n",
    "#                             subsample_length=1))\n",
    "#     # we use max pooling:\n",
    "#     model.add(GlobalMaxPooling1D())\n",
    "#     # We add a vanilla hidden layer:\n",
    "#     model.add(Dense(hidden_dims))\n",
    "#     model.add(Dropout(0.0))\n",
    "#     model.add(Activation('relu'))\n",
    "#     model.add(Dense(hidden_dims))\n",
    "#     model.add(Dropout(0.0))\n",
    "#     model.add(Activation('relu'))\n",
    "\n",
    "#     if no_merge:\n",
    "#         model.add(Dense(1))\n",
    "#         model.add(Activation('sigmoid'))\n",
    "#         model.compile(loss='binary_crossentropy',\n",
    "#                       optimizer='adam',\n",
    "#                       metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "\n",
    "# def create_model_text(no_merge = False):\n",
    "#     # CNN/LSTM https://github.com/fchollet/keras/blob/master/examples/imdb_cnn_lstm.py\n",
    "#     # Embedding\n",
    "#     embedding_size = 128\n",
    "\n",
    "#     # Convolution\n",
    "#     filter_length = 5\n",
    "#     nb_filter = 64\n",
    "#     pool_length = 4\n",
    "\n",
    "#     # LSTM\n",
    "#     lstm_output_size = 70\n",
    "\n",
    "#     '''\n",
    "#     Note:\n",
    "#     batch_size is highly sensitive.\n",
    "#     Only 2 epochs are needed as the dataset is very small.\n",
    "#     '''\n",
    "    \n",
    "#     model = Sequential()\n",
    "#     model.add(Embedding(max_features, embedding_size, input_length=maxlen))\n",
    "#     model.add(Dropout(0.25))\n",
    "#     model.add(Convolution1D(nb_filter=nb_filter,\n",
    "#                             filter_length=filter_length,\n",
    "#                             border_mode='valid',\n",
    "#                             activation='relu',\n",
    "#                             subsample_length=1))\n",
    "#     model.add(MaxPooling1D(pool_length=pool_length))\n",
    "#     model.add(LSTM(lstm_output_size))\n",
    "    \n",
    "    \n",
    "#     if no_merge:\n",
    "#         model.add(Dense(1))\n",
    "#         model.add(Activation('sigmoid'))\n",
    "\n",
    "#         model.compile(loss='binary_crossentropy',\n",
    "#                       optimizer='adam',\n",
    "#                       metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "        \n",
    "        \n",
    "def create_model_num(no_merge = False):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(4, input_dim=36, activation='relu', activity_regularizer=activity_l2(0.0001)))\n",
    "    if no_merge:\n",
    "        model.add(Dense(1, input_dim=36, activation='sigmoid',init='zero', activity_regularizer=activity_l2(0.0001)))\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                          optimizer='adam',\n",
    "                          metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_model_merge_layer():\n",
    "    model_text = create_model_text()\n",
    "    model_num = create_model_num()\n",
    "    merged = Merge([model_text, model_num], mode='concat')\n",
    "    final_model = Sequential()\n",
    "    final_model.add(merged)\n",
    "    print(final_model.layers[-1].output_shape)\n",
    "    final_model.add(Dense(1, activation='sigmoid'))\n",
    "    final_model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "embedding_dims = 100\n",
    "batch_size = 32\n",
    "nb_epoch = 3\n",
    "\n",
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test, batch_size = batch_size, nb_epoch = nb_epoch, verbose = 1):\n",
    "    \"\"\"Calculate and print ROC score from a set of X and y\"\"\"\n",
    "    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, \n",
    "             validation_data=(X_test, y_test), verbose = verbose)\n",
    "    pred_yp = model.predict(X_test)\n",
    "    roc = metrics.roc_auc_score(y_test , pred_yp)\n",
    "    print(roc)\n",
    "    return roc\n",
    "\n",
    "def forecast_performace(X_text, X_num, train_index, test_index, y, model_type = \"text\"):\n",
    "    \"\"\"Train_test split using year; Calculate and print performance score\"\"\"\n",
    "    print(\"Running Model\")\n",
    "    model = None # Clearing the NN.\n",
    "    \n",
    "    if model_type == \"text\":\n",
    "        model = create_model_text(no_merge=True)\n",
    "        X_train = X_text[train_index]\n",
    "        y_train = y[train_index]\n",
    "        X_test = X_text[test_index]\n",
    "        y_test = y[test_index]\n",
    "        train_and_evaluate_model(model, X_train, y_train, X_test, y_test)\n",
    "        \n",
    "    if model_type == \"num\":\n",
    "        model = create_model_num(no_merge=True)\n",
    "        X_train = X_num[train_index]\n",
    "        y_train = y[train_index]\n",
    "        X_test = X_num[test_index]\n",
    "        y_test = y[test_index]\n",
    "        train_and_evaluate_model(model, X_train, y_train, X_test, y_test, verbose = 0)\n",
    "\n",
    "    if model_type == \"merge\":\n",
    "        model = create_model_merge_layer()\n",
    "        X_train = [X_text[train_index], X_num[train_index]]\n",
    "        y_train = y[train_index]\n",
    "        X_test = [X_text[test_index], X_num[test_index]]\n",
    "        y_test =  y[test_index]\n",
    "        train_and_evaluate_model(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    \n",
    "\n",
    "def kfold_performace(X_text, X_num, y, n_folds = 2):\n",
    "    \"\"\"Calculate and print average stratified K-fold performance score\"\"\"\n",
    "    skf = StratifiedKFold(y, n_folds=n_folds, shuffle=True)\n",
    "    performance_scores = []\n",
    "    for i, (train_index, test_index) in enumerate(skf):\n",
    "        print(\"Running Fold\", i+1, \"/\", n_folds)\n",
    "        model = None # Clearing the NN.\n",
    "        model = create_model_merge_layer()\n",
    "        performance_scores.append(train_and_evaluate_model(model, X_text, X_num, y, train_index, test_index, \n",
    "                                                          just_text=False))\n",
    "    print(sum(performance_scores)/n_folds)\n",
    "\n",
    "#kfold_performace(X, y, n_folds = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model\n",
      "Build model...\n",
      "(None, 5000, 100)\n",
      "(None, 1, 100)\n",
      "(None, 1)\n",
      "Train on 50286 samples, validate on 14713 samples\n",
      "Epoch 1/3\n",
      "50286/50286 [==============================] - 176s - loss: 0.1026 - acc: 0.9942 - val_loss: 0.0180 - val_acc: 0.9984\n",
      "Epoch 2/3\n",
      "50286/50286 [==============================] - 176s - loss: 0.0598 - acc: 0.9950 - val_loss: 0.0141 - val_acc: 0.99849 - E - ETA: \n",
      "Epoch 3/3\n",
      "50286/50286 [==============================] - 175s - loss: 0.0440 - acc: 0.9950 - val_loss: 0.0137 - val_acc: 0.9984\n",
      "0.74122075476\n"
     ]
    }
   ],
   "source": [
    "forecast_performace(X_text, X_num, train_index, test_index, y, \"text\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "66px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
