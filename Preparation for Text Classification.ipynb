{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from keras.preprocessing.text import Tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load 10K raw file and output textual features for classification using sklearn and keras.  \n",
    "Inputs: \n",
    "- data/10k/10k_raw.pickle: a pandas pickle file containing gvkey, fyear, and MD&A section \n",
    "\n",
    "Outputs:\n",
    "- data/10k/10k_index.csv: an index file that preserves the sequence \n",
    "- data/10k/X_keras_unigram.npy: unigram sequences for keras\n",
    "- data/10k/word_map.pickle: a dictionary to map numbers to words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load 10k raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw text data\n"
     ]
    }
   ],
   "source": [
    "print('Loading raw text data')\n",
    "sec_10k = pd.read_pickle(\"data/10k/10k_raw.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save gvkey-fyear indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sec10k_index = sec_10k[['gvkey','fyear']]\n",
    "sec10k_index.to_csv(\"data/10k/10k_index.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use keras to pre-process text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 5000 # max number of words to include (remove lower frequency words)\n",
    "mda_text_list = sec_10k['mda_text'].tolist()\n",
    "del sec_10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenize the raw text\n",
    "tokenizer = Tokenizer(nb_words=max_features)\n",
    "tokenizer.fit_on_texts(mda_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# tokenizer.word_index # a dictionary that maps word to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X is a matrix, each row is sequence of word ids\n",
    "X = tokenizer.texts_to_sequences(mda_text_list) \n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the tokenized sequence to disk\n",
    "np.save(\"data/10k/X_keras_unigram.npy\", X)\n",
    "# np.save(\"data/10k/X_keras_unigram_20000.npy\", X)\n",
    "# save the word mapping to disk\n",
    "pickle.dump(tokenizer.word_index, file=open(\"data/10k/word_map.pickle\", 'wb'))\n",
    "# pickle.dump(tokenizer.word_index, file=open(\"data/10k/word_map_20000.pickle\", 'wb'))\n",
    "# save the trained tokenizer\n",
    "# pickle.dump(tokenizer, file=open(\"data/10k/tokenizer.pickle\", 'wb'))\n",
    "# pickle.dump(tokenizer, file=open(\"data/10k/tokenizer_20000.pickle\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_tfidf = tokenizer.texts_to_matrix(mda_text_list, mode = 'tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_tfidf = np.array(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(\"data/10k/X_tfidf.npy\", X_tfidf)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "138px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
